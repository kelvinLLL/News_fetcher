{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nzWB8C0xeFv"
      },
      "source": [
        "From our nlp course project\n",
        "https://github.com/Aaalan-Zhang/nlp-from-scratch-assignment-fall2024/blob/main/crawler/tests/0_webpage_parsing.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raw4MUTJxt4N",
        "outputId": "4ffa7b71-f681-4f91-c45e-a2775a348a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os\n",
        "os.listdir(\"./drive/MyDrive/Proj/News_fetcher\")\n",
        "\n",
        "os.chdir(\"./drive/MyDrive/Proj/News_fetcher\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVhN4FPwyG3z"
      },
      "source": [
        "# Crawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtrVwjjw8vwZ"
      },
      "outputs": [],
      "source": [
        "# # === Install Chrome and ChromeDriver v137 ===\n",
        "# !apt-get remove -y chromium-browser google-chrome-stable || true\n",
        "# !rm -rf /usr/bin/google-chrome /usr/bin/chromedriver137\n",
        "\n",
        "# !wget -q https://storage.googleapis.com/chrome-for-testing-public/137.0.7151.119/linux64/chrome-linux64.zip\n",
        "# !unzip -q chrome-linux64.zip\n",
        "# !mv chrome-linux64/chrome /usr/bin/google-chrome\n",
        "\n",
        "# !wget -q https://storage.googleapis.com/chrome-for-testing-public/137.0.7151.119/linux64/chromedriver-linux64.zip\n",
        "# !unzip -q chromedriver-linux64.zip\n",
        "# !mv chromedriver-linux64/chromedriver /usr/bin/chromedriver137\n",
        "# !chmod +x /usr/bin/google-chrome /usr/bin/chromedriver137"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btqfGdkVAbXO",
        "outputId": "8532dc80-60cf-4538-a89f-a175a6b73950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q google-colab-selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW1XcHw1yIrm"
      },
      "source": [
        "## Original Crawler from 711"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4P5o7T0xbQA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "from urllib.parse import urljoin\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "user_agents = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
        "]\n",
        "warnings.filterwarnings('ignore', category=InsecureRequestWarning)\n",
        "\n",
        "# Define headers for requests\n",
        "headers = {'User-Agent': random.choice(user_agents)}\n",
        "session = requests.Session()\n",
        "\n",
        "# Fetch the page text with retry mechanism\n",
        "def fetch_page_text(url, retries=3, timeout=5):\n",
        "    attempt = 0\n",
        "    while attempt < retries:\n",
        "        try:\n",
        "            # Send a GET request to the URL with a timeout\n",
        "            response = session.get(url, timeout=timeout, headers=headers, verify=False)\n",
        "            response.raise_for_status()  # Raise an error for bad responses\n",
        "\n",
        "            # Parse the page content\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "            # Extract and return the text from the page and the soup object\n",
        "            return soup.get_text(separator='\\n', strip=True), soup\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            attempt += 1\n",
        "            time.sleep(1)  # Optional: wait before retrying\n",
        "\n",
        "        if attempt == retries:\n",
        "            return None, None\n",
        "\n",
        "# Extract all sublinks (anchor tags) and handle relative URLs\n",
        "def extract_sublinks(soup, base_url):\n",
        "    sublinks = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        sublink = link['href']\n",
        "        # Convert relative URLs to absolute URLs using urljoin\n",
        "        full_url = urljoin(base_url, sublink)\n",
        "        sublinks.append(full_url)\n",
        "    return sublinks\n",
        "\n",
        "# Crawl through the URLs and fetch the data with retry\n",
        "def crawl_urls(url_list):\n",
        "    results = {}\n",
        "    sublinks_data = []  # List to store (parent_url, sublink) pairs\n",
        "    for index, url in enumerate(url_list):\n",
        "        print(f\"Fetching: {url}\")\n",
        "        text, soup = fetch_page_text(url)\n",
        "        if text and soup:\n",
        "            # Store the crawled text\n",
        "            results[url] = text\n",
        "            # Extract sublinks\n",
        "            sublinks = extract_sublinks(soup, url)\n",
        "            for sublink in sublinks:\n",
        "                sublinks_data.append((url, sublink))\n",
        "        else:\n",
        "            print(f\"Failed to parse URL at index: {index}, URL: {url}\")\n",
        "    return results, sublinks_data\n",
        "\n",
        "# Save the crawled text data to .txt files\n",
        "def save_crawled_data(crawled_data, urls):\n",
        "    for index, url in enumerate(urls):\n",
        "        # Fetch the text corresponding to the URL from the crawled_data dictionary\n",
        "        text = crawled_data.get(url, \"\")\n",
        "\n",
        "        # Remove all newline characters from the text\n",
        "        cleaned_text = text.replace('\\n', ' ')\n",
        "\n",
        "        # Define the file name with the index from the URLs list\n",
        "        output_file = f\"/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/crawled/crawled_text_data/{index}.txt\"\n",
        "\n",
        "        # Save the cleaned text to the file\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            file.write(cleaned_text)\n",
        "\n",
        "# Save sublinks to a CSV file\n",
        "def save_sublinks_to_csv(sublinks_data, output_csv):\n",
        "    df = pd.DataFrame(sublinks_data, columns=['Parent URL', 'Sublink'])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/raw/raw_csv_data/data_source.csv'\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract non-empty URLs from the 'Source URL' column\n",
        "    urls = data[data['Select'] == 'Webpage']['Source URL'].dropna().unique()\n",
        "\n",
        "    # Start crawling the URLs\n",
        "    crawled_data, sublinks_data = crawl_urls(urls)\n",
        "\n",
        "    # Save the crawled text data\n",
        "    save_crawled_data(crawled_data, urls)\n",
        "\n",
        "    # Save sublinks to a CSV file\n",
        "    sublinks_output_file = \"/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/crawled/crawled_sublinks.csv\"\n",
        "    save_sublinks_to_csv(sublinks_data, sublinks_output_file)\n",
        "\n",
        "    print(\"Crawling complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qBQHNgJyhH2"
      },
      "source": [
        "## 参考消息"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MCo8lo9vy-fM",
        "outputId": "9f151552-df67-4071-e43f-7ab5b5197017"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2025-06-19'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "url = \"https://www.cankaoxiaoxi.com/#/detailsPage/tt/329d19f3dfae4b50b524f428cd62637c/1/2025-06-18%2009:46?childrenAlias=undefined\"\n",
        "prefix = \"/content/drive/MyDrive/Proj/News_fetcher/\"\n",
        "\n",
        "\n",
        "# Get today's date\n",
        "suffix = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "suffix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW-t_orAfEos"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25cBDCwlzCUm",
        "outputId": "fda56c1a-5279-4b46-bf87-d30f9307decd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching: https://www.cankaoxiaoxi.com/#/detailsPage/tt/329d19f3dfae4b50b524f428cd62637c/1/2025-06-18%2009:46?childrenAlias=undefined\n",
            "response is :\n",
            "b'<!doctype html><html><head><meta charset=\"UTF-8\"/><meta name=\"keywords\" content=\"\"><meta name=\"description\" content=\"\"><meta name=\"apple-mobile-web-app-capable\" content=\"yes\"/><meta name=\"apple-touch-fullscreen\" content=\"yes\"/><meta name=\"apple-mobile-web-app-status-bar-style\" content=\"black\"/><title>\\xe5\\x8f\\x82\\xe8\\x80\\x83\\xe6\\xb6\\x88\\xe6\\x81\\xaf</title><script defer=\"defer\" src=\"/statics/ckxx-website/static/js/main.4651e5cf.js\"></script><link href=\"/statics/ckxx-website/static/css/main.dd0b7dd0.css\" rel=\"stylesheet\"></head><script>window._czc = window._czc || []; //\\xe7\\xbb\\x9f\\xe8\\xae\\xa1\\xe7\\x94\\xa8\\xe5\\x85\\xa8\\xe5\\xb1\\x80\\xe5\\x8f\\x98\\xe9\\x87\\x8f\\n    let targetWidth = 1400;\\n    // let scale = document.documentElement.clientWidth / targetWidth;\\n    let scale = window.screen.width / targetWidth;\\n    const viewport = document.createElement(\\'meta\\');\\n    viewport.setAttribute(\\'name\\', \\'viewport\\');\\n    viewport.content = `width=device-width,initial-scale=${scale},maximum-scale=${scale},user-scalable=0`;\\n    document.getElementsByTagName(\\'head\\')[0].appendChild(viewport);\\n               \\n    function checkBrowser() {\\n        var browser = {\\n          versions: function () {\\n            var u = navigator.userAgent, app = navigator.appVersion;\\n            return {\\n              //\\xe7\\xa7\\xbb\\xe5\\x8a\\xa8\\xe7\\xbb\\x88\\xe7\\xab\\xaf\\xe6\\xb5\\x8f\\xe8\\xa7\\x88\\xe5\\x99\\xa8\\xe7\\x89\\x88\\xe6\\x9c\\xac\\xe4\\xbf\\xa1\\xe6\\x81\\xaf                 \\n              trident: u.indexOf(\\'Trident\\') > -1, //IE\\xe5\\x86\\x85\\xe6\\xa0\\xb8                 \\n              presto: u.indexOf(\\'Presto\\') > -1, //opera\\xe5\\x86\\x85\\xe6\\xa0\\xb8                 \\n              webKit: u.indexOf(\\'AppleWebKit\\') > -1, //\\xe8\\x8b\\xb9\\xe6\\x9e\\x9c\\xe3\\x80\\x81\\xe8\\xb0\\xb7\\xe6\\xad\\x8c\\xe5\\x86\\x85\\xe6\\xa0\\xb8                 \\n              gecko: u.indexOf(\\'Gecko\\') > -1 && u.indexOf(\\'KHTML\\') == -1, //\\xe7\\x81\\xab\\xe7\\x8b\\x90\\xe5\\x86\\x85\\xe6\\xa0\\xb8                 \\n              mobile: !!u.match(/AppleWebKit.*Mobile.*/) || !!u.match(/AppleWebKit/), //\\xe6\\x98\\xaf\\xe5\\x90\\xa6\\xe4\\xb8\\xba\\xe7\\xa7\\xbb\\xe5\\x8a\\xa8\\xe7\\xbb\\x88\\xe7\\xab\\xaf                 \\n              ios: !!u.match(/\\\\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios\\xe7\\xbb\\x88\\xe7\\xab\\xaf                 \\n              android: u.indexOf(\\'Android\\') > -1, //android\\xe7\\xbb\\x88\\xe7\\xab\\xaf\\xe6\\x88\\x96\\xe8\\x80\\x85uc\\xe6\\xb5\\x8f\\xe8\\xa7\\x88\\xe5\\x99\\xa8                 \\n              iPhone: u.indexOf(\\'iPhone\\') > -1, //\\xe6\\x98\\xaf\\xe5\\x90\\xa6\\xe4\\xb8\\xbaiPhone\\xe6\\x88\\x96\\xe8\\x80\\x85QQHD\\xe6\\xb5\\x8f\\xe8\\xa7\\x88\\xe5\\x99\\xa8                 \\n              iPad: u.indexOf(\\'iPad\\') > -1, //\\xe6\\x98\\xaf\\xe5\\x90\\xa6iPad                 \\n              webApp: u.indexOf(\\'Safari\\') == -1 //\\xe6\\x98\\xaf\\xe5\\x90\\xa6web\\xe5\\xba\\x94\\xe8\\xaf\\xa5\\xe7\\xa8\\x8b\\xe5\\xba\\x8f\\xef\\xbc\\x8c\\xe6\\xb2\\xa1\\xe6\\x9c\\x89\\xe5\\xa4\\xb4\\xe9\\x83\\xa8\\xe4\\xb8\\x8e\\xe5\\xba\\x95\\xe9\\x83\\xa8             \\n            };\\n          }(),\\n          language: (navigator.browserLanguage || navigator.language).toLowerCase()\\n        }\\n        if (browser.versions.android || browser.versions.iPhone || browser.versions.iPad) {\\n          window.location.href = \"https://www.media.xinhuamm.net/statics/h5-news-media/index.html#/?siteId=2b0cbbb1d563414393513f147f7e9799\"\\n        }\\n    }\\n    checkBrowser();</script><body><div id=\"root\"></div><div style=\"display:none\"><script src=\"https://s4.cnzz.com/z_stat.php?id=1281256312&web_id=1281256312\"></script></div></body></html>'\n",
            "Crawling complete!\n"
          ]
        }
      ],
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# import random\n",
        "# import warnings\n",
        "# from urllib.parse import urljoin\n",
        "# from urllib3.exceptions import InsecureRequestWarning\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "# from bs4 import BeautifulSoup\n",
        "# import time\n",
        "\n",
        "# user_agents = [\n",
        "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "#     'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
        "# ]\n",
        "# warnings.filterwarnings('ignore', category=InsecureRequestWarning)\n",
        "\n",
        "# # Define headers for requests\n",
        "# headers = {'User-Agent': random.choice(user_agents)}\n",
        "# session = requests.Session()\n",
        "\n",
        "# # Fetch the page text with retry mechanism\n",
        "# def fetch_page_text(url, retries=3, timeout=5):\n",
        "#     attempt = 0\n",
        "#     while attempt < retries:\n",
        "#         try:\n",
        "#             # Send a GET request to the URL with a timeout\n",
        "#             response = session.get(url, timeout=timeout, headers=headers, verify=False)\n",
        "#             response.raise_for_status()  # Raise an error for bad responses\n",
        "\n",
        "#             # Parse the page content\n",
        "#             soup = BeautifulSoup(response.content, 'lxml')\n",
        "#             print(\"response is :\")\n",
        "#             print(response.content)\n",
        "#             # Extract and return the text from the page and the soup object\n",
        "#             return soup.get_text(separator='\\n', strip=True), soup\n",
        "\n",
        "#         except requests.exceptions.RequestException as e:\n",
        "#             attempt += 1\n",
        "#             time.sleep(1)  # Optional: wait before retrying\n",
        "\n",
        "#         if attempt == retries:\n",
        "#             return None, None\n",
        "\n",
        "# # def fetch_page_text_selenium(url, retries=3, timeout=5):\n",
        "# #     # Setup headless Chrome\n",
        "# #     options = Options()\n",
        "# #     options.add_argument('--headless')\n",
        "# #     options.add_argument('--disable-gpu')\n",
        "\n",
        "# #     driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# #     # Open page\n",
        "# #     driver.get(\"https://your_url_here.com\")\n",
        "# #     time.sleep(3)  # wait for JS to load; adjust as needed\n",
        "\n",
        "# #     # Get rendered HTML\n",
        "# #     html = driver.page_source\n",
        "# #     soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "# #     # Now you can extract dynamically loaded content\n",
        "# #     print(soup.prettify())\n",
        "\n",
        "# #     driver.quit()\n",
        "\n",
        "# # Extract all sublinks (anchor tags) and handle relative URLs\n",
        "# def extract_sublinks(soup, base_url):\n",
        "#     sublinks = []\n",
        "#     for link in soup.find_all('a', href=True):\n",
        "#         sublink = link['href']\n",
        "#         # Convert relative URLs to absolute URLs using urljoin\n",
        "#         full_url = urljoin(base_url, sublink)\n",
        "#         sublinks.append(full_url)\n",
        "#     return sublinks\n",
        "\n",
        "# # Crawl through the URLs and fetch the data with retry\n",
        "# def crawl_urls(url_list):\n",
        "#     results = {}\n",
        "#     sublinks_data = []  # List to store (parent_url, sublink) pairs\n",
        "#     for index, url in enumerate(url_list):\n",
        "#         print(f\"Fetching: {url}\")\n",
        "#         text, soup = fetch_page_text(url)\n",
        "#         if text and soup:\n",
        "#             # Store the crawled text\n",
        "#             results[url] = text\n",
        "#             # Extract sublinks\n",
        "#             sublinks = extract_sublinks(soup, url)\n",
        "#             for sublink in sublinks:\n",
        "#                 sublinks_data.append((url, sublink))\n",
        "#         else:\n",
        "#             print(f\"Failed to parse URL at index: {index}, URL: {url}\")\n",
        "#     return results, sublinks_data\n",
        "\n",
        "# # Save the crawled text data to .txt files\n",
        "# def save_crawled_data(crawled_data, urls, prefix):\n",
        "#     for index, url in enumerate(urls):\n",
        "#         # Fetch the text corresponding to the URL from the crawled_data dictionary\n",
        "#         text = crawled_data.get(url, \"\")\n",
        "\n",
        "#         # Remove all newline characters from the text\n",
        "#         cleaned_text = text.replace('\\n', ' ')\n",
        "\n",
        "#         # Define the file name with the index from the URLs list\n",
        "#         output_file = f\"{prefix}data/crawled_data/crawled_text_data/{index}.txt\"\n",
        "#         output_dir = os.path.dirname(output_file)\n",
        "\n",
        "#         # Check if directory exists, and create if not\n",
        "#         if not os.path.exists(output_dir):\n",
        "#             os.makedirs(output_dir)\n",
        "\n",
        "#         # Save the cleaned text to the file\n",
        "#         with open(output_file, 'w', encoding='utf-8') as file:\n",
        "#             file.write(cleaned_text)\n",
        "\n",
        "# # Save sublinks to a CSV file\n",
        "# def save_sublinks_to_csv(sublinks_data, output_csv):\n",
        "#     df = pd.DataFrame(sublinks_data, columns=['Parent URL', 'Sublink'])\n",
        "#     df.to_csv(output_csv, index=False)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # file_path = '/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/raw/raw_csv_data/data_source.csv'\n",
        "#     # data = pd.read_csv(file_path)\n",
        "\n",
        "#     # # Extract non-empty URLs from the 'Source URL' column\n",
        "#     # urls = data[data['Select'] == 'Webpage']['Source URL'].dropna().unique()\n",
        "\n",
        "#     url = \"https://www.cankaoxiaoxi.com/#/detailsPage/tt/329d19f3dfae4b50b524f428cd62637c/1/2025-06-18%2009:46?childrenAlias=undefined\"\n",
        "#     prefix = \"/content/drive/MyDrive/Proj/News_fetcher/\"\n",
        "\n",
        "#     urls = [url]\n",
        "#     # Start crawling the URLs\n",
        "#     crawled_data, sublinks_data = crawl_urls(urls)\n",
        "\n",
        "#     # Save the crawled text data\n",
        "#     save_crawled_data(crawled_data, urls, prefix)\n",
        "\n",
        "#     # Save sublinks to a CSV file\n",
        "#     sublinks_output_file = prefix + \"data/crawled_data/crawled_sublinks.csv\"\n",
        "#     save_sublinks_to_csv(sublinks_data, sublinks_output_file)\n",
        "\n",
        "#     print(\"Crawling complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "73IgXPX_5007",
        "outputId": "d2bb2d1c-bec5-44ee-c7e2-a5bd295c16cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"d5494448-c0de-4e03-b8d0-db3e8e510f0b-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"d5494448-c0de-4e03-b8d0-db3e8e510f0b-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"d5494448-c0de-4e03-b8d0-db3e8e510f0b-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"d5494448-c0de-4e03-b8d0-db3e8e510f0b-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtdWMib6NNcI"
      },
      "source": [
        "### Workable for 参考消息\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrRYZYBofK84"
      },
      "outputs": [],
      "source": [
        "def wait_for_stable_page(driver, timeout=10, interval=1.5):\n",
        "    \"\"\"Waits for the whole text content to stop changing.\"\"\"\n",
        "    stable_counter = 0\n",
        "    prev_text = \"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            html = driver.page_source\n",
        "            curr_text = html.strip()\n",
        "        except:\n",
        "            curr_text = \"\"\n",
        "\n",
        "        if curr_text == prev_text and len(curr_text) > 0:\n",
        "            stable_counter += 1\n",
        "            if stable_counter >= 2:  # Stable twice in a row\n",
        "                return curr_text\n",
        "        else:\n",
        "            stable_counter = 0\n",
        "        prev_text = curr_text\n",
        "        time.sleep(interval)\n",
        "        # print(len(curr_text))\n",
        "    return\n",
        "    raise TimeoutException(\"detailsPage content did not stabilize in time.\")\n",
        "\n",
        "def safe_get(driver, url, max_retries=5, wait_between=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            return  # success\n",
        "        except TimeoutException:\n",
        "            print(f\"[Attempt {attempt+1}] Timeout loading: {url}\")\n",
        "            time.sleep(wait_between)\n",
        "        except (WebDriverException, InvalidSessionIdException) as e:\n",
        "            print(f\"[{attempt+1}] WebDriver error: {e}. Restarting driver...\")\n",
        "            try:\n",
        "                driver.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            driver = get_fresh_driver()\n",
        "            time.sleep(2)\n",
        "    raise TimeoutException(f\"Failed to load {url} after {max_retries} attempts.\")\n",
        "\n",
        "def get_fresh_driver():\n",
        "    try:\n",
        "        print(\"Initializing Chromedriver\")\n",
        "        driver = gs.Chrome()\n",
        "        time.sleep(2)  # Let browser settle\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(\"Driver failed to initialize:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZv9uoE27FG-"
      },
      "outputs": [],
      "source": [
        "def restart_driver_and_get(url, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        driver = get_fresh_driver()\n",
        "        if driver is None:\n",
        "            continue\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            return driver\n",
        "        except WebDriverException as e:\n",
        "            print(f\"[{attempt+1}] Error loading {url}: {e}\")\n",
        "            try:\n",
        "                driver.quit()\n",
        "            except:\n",
        "                pass\n",
        "            time.sleep(2)\n",
        "    print(\"All attempts failed.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEi7QS49SWs-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume these are your lists\n",
        "# sublinks = ['https://example.com/page1', 'https://example.com/page2', ...]\n",
        "# contents = ['Text content of page 1...', 'Text content of page 2...', ...]\n",
        "\n",
        "# Combine into a DataFrame\n",
        "def save_to_csv(sublinks, contents, file_name, tags=None):\n",
        "    df = pd.DataFrame({\n",
        "        'ind': range(len(sublinks)),\n",
        "        'url': sublinks,\n",
        "        'content': contents\n",
        "    })\n",
        "    if tags:\n",
        "      df['tag'] = tags\n",
        "    # Save to CSV\n",
        "    df.to_csv(f'{prefix}data/crawled_data/{suffix}_{file_name}', index=False, encoding='utf-8-sig')  # Use utf-8-sig for Chinese\n",
        "\n",
        "def load_csv(file_name):\n",
        "    return pd.read_csv(f'{prefix}data/crawled_data/{suffix}_{file_name}', encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8qX-uIndZJ6"
      },
      "source": [
        "### Index Page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9SRcPENTg3K"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import (\n",
        "    WebDriverException,\n",
        "    TimeoutException,\n",
        "    NoSuchElementException,\n",
        "    InvalidSessionIdException\n",
        ")\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import google_colab_selenium as gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "ZcfsRJHpdVDn",
        "outputId": "4659f609-9462-41b0-85ef-f4368234538a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cankaoxiaoxi_index_visiting' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3049528481.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# print(soup.get_text()[:10000])  # preview 1000 chars of text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcankaoxiaoxi_index_visiting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# driver.quit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msave_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"index_contents.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cankaoxiaoxi_index_visiting' is not defined"
          ]
        }
      ],
      "source": [
        "index_url = \"https://www.cankaoxiaoxi.com/#/index\"\n",
        "sublinks = []\n",
        "contents = []\n",
        "\n",
        "\n",
        "\n",
        "# # Parse after content loads\n",
        "# html = driver.page_source\n",
        "# soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "# # Print readable content\n",
        "# print(\"Title:\", driver.title)\n",
        "# print(soup.get_text()[:10000])  # preview 1000 chars of text\n",
        "\n",
        "cankaoxiaoxi_index_visiting(sublinks, contents, index_url)\n",
        "# driver.quit()\n",
        "save_to_csv(sublinks, contents, \"index_contents.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmr0N_ElWHv6"
      },
      "source": [
        "### generalColumns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfM_Zkrgc0L2"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import (\n",
        "    WebDriverException,\n",
        "    TimeoutException,\n",
        "    NoSuchElementException,\n",
        "    InvalidSessionIdException\n",
        ")\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import google_colab_selenium as gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pOu6_LinWKsB"
      },
      "outputs": [],
      "source": [
        "sublinks = []\n",
        "contents = []\n",
        "tags = []\n",
        "generalColumnsDict = {\n",
        "    '观点': \"https://www.cankaoxiaoxi.com/#/generalColumns/guandian\",\n",
        "}\n",
        "for k, v in generalColumnsDict.items():\n",
        "    cankaoxiaoxi_generalColumns_visiting(sublinks, contents, tags, k, v, \"templateModule\")\n",
        "\n",
        "\n",
        "save_to_csv(sublinks, contents, \"index_contents.csv\", tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L1Jcx48guk0S",
        "outputId": "e6e8c750-7d81-429f-bc7c-ebd598cea260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['参考消息第一关注中国国际观点锐参考体育健康科技应用文化旅游参考漫谈参考智库军事民族品牌参考人物产经地方译名文娱我的位置：首页>观点>新闻详情澳前外长：“澳大利亚应期待澳英美联盟崩溃”2025-06-18 16:23:1918573分享到：用微信扫描二维码参考消息网6月18日报道 世界报业辛迪加网站6月16日刊登题为《澳大利亚应期待澳英美联盟的崩溃》的文章，作者为澳大利亚前外交部长加雷思·埃文斯。内容编译如下：根据2021年达成的澳英美联盟合作协议，美国和英国将在30年内向澳大利亚提供至少八艘核动力潜艇。目前，美国国防部正在对该协议进行审查。如果协议破裂，那些乐见三国同盟深化的人士——尤其是盼着澳大利亚给资金短缺、效率低下的美英海军造船厂输送数千亿美元资金的人——不出意外会陷入焦虑。但对澳大利亚而言，澳英美联盟的瓦解可喜可贺。毕竟，关于协议承诺的潜艇能否按时交付，始终存在不确定性。根据计划，美国应从2032年起交付三艘(或五艘)“弗吉尼亚”级潜艇，另外五艘新设计的“澳英美联盟”级核潜艇则计划于本世纪40年代初服役。但美英两国受本国潜艇建造目标的影响，工业产能已处于饱和状态，且双方均明确拥有协议退出权。一些分析人士认为，美国国防部此次审查不过是特朗普式的敲诈伎俩，意在从澳大利亚榨取更多财政投入。这种解读虽能让部分澳大利亚人感到宽慰，但其实是错误的。即使一切顺利——从“弗吉尼亚”级潜艇的交付到英国新潜艇的建造，既没有遇到人力资源瓶颈，也没有发生成本超支——澳大利亚仍需等待数十年才能接收最后一艘潜艇。但澳大利亚老旧的“柯林斯”级潜艇舰队已进入退役期，这一时间表构成了严峻挑战：在此期间，我们该如何填补能力缺口？澳大利亚本应在一开始就通过成本效益分析否决该项目，但政治领导人急于达成这项协议，并没有认真审查内容。即使承认核动力潜艇在速度与续航能力上有着显著优势，且乐观地假定其水下隐蔽性在服役期内不会受到技术挑战，最终的舰队规模仍明显不符合国防需求。考虑到常规操作限制，澳大利亚任何时候最多只能部署两艘此类潜艇。在这种情况下，究竟能完成多少情报收集、群岛要塞保护、海上航道护卫，甚至远程威慑任务？此外，该项目天文数字般的成本将导致澳大利亚难以获取其他关键能力，如先进无人机、导弹、飞机和网络防御系统，而这些能力正在重新定义现代战争。正如澳大利亚前总理保罗·基廷所言，美国若退出协议，将成为“华盛顿拯救澳大利亚的时刻”。这一观点的核心在于澳英美联盟对澳大利亚主权的负面影响。美国同意该协议的根本原因是其符合自身战略利益，而非澳方利益。我们不能轻信美国会向澳大利亚转让如此敏感的技术。澳英美联盟及其相关承诺带给澳大利亚的，只是在我们背上画下更多靶子。荒谬的是，我们为了抵御军事威胁，投入巨额资金打造新能力，但这些威胁会出现的最大原因，恰恰是我们拥有了这种能力，而且还会使用它们来支持美国。然而，我们无法保证在自己有需要的时候获得美国的支持。若澳英美联盟项目最终破裂，澳大利亚仍有望在合理期限内为老化的潜艇舰队寻得替代方案——不仅成本可能更低，还能保留真正的自主权——例如从其他国家采购现成技术。我们甚至可以考虑，重新转向被澳英美联盟协议冷落的法国。但更好的国防选择或许是正视军事技术的最新变革：相较于寥寥数艘有人驾驶潜艇，由自主空中、导弹、水下及网络探测能力构成的综合体系，更能有效守护澳大利亚广袤的大陆与海洋疆域。如今，正是跳出美澳同盟框架重新思考的最佳时机。（编译/王栋栋）更多新闻澳前外长：“澳大利亚应期待澳英美联盟崩溃”2025-06-18 16:23美媒：以色列正“不受制约”加紧重塑中东格局2025-06-18 11:09美媒：政治暴力频发凸显美国两极分化2025-06-17 17:05港大教授：教育有助消弭中美之间的误解2025-06-17 15:39美专家：“美国优先”成日本最大担忧2025-06-17 15:38美媒：G7凸显美国与盟友裂痕日益加深2025-06-17 11:24日媒：美一家独大导致G7难以发挥作用2025-06-17 11:23西媒：“文化战”加剧美国社会分裂2025-06-16 19:26美专家：为什么美国制造业回流并不容易2025-06-16 18:42美国外交关系协会名誉会长理查德·哈斯：中东冲突进入新阶段2025-06-16 18:42点击加载更多专题 2025企业品牌国际传播论坛 二战回眸与反思 第十五届北京国际电影节 2025世界看两会 聚焦俄乌冲突三周年排行榜外媒分析：以色列为何此时对伊朗“动手”美国：两场“大戏”即将同时上场！外媒：伊朗和以色列军队孰强孰弱？突发！以色列对伊朗发动打击美媒：美政府取消四国数十万移民合法身份中国成功斡旋，“化解宿敌”美媒：印航波音客机坠毁原因何在？伊朗：“友好”国家已就以色列动武向德黑兰发出警报针对以色列袭击伊朗，国际社会纷纷发声西媒：论中国的长远思维与五年规划关于参考消息|版权声明|广告服务|联系我们国新网备2012001\\xa0\\xa0 互联网新闻信息服务许可证\\xa0\\xa0京ICP备11013708\\xa0\\xa0京公网安备11040202440054\\xa0\\xa0互联网药品信息服务资格证书(京)-非经营性-2019-0121\\xa0\\xa0广播电视节目制作经营许可证\\xa0\\xa0中华人民共和国增值电信业务经营许可证\\xa0\\xa0高新技术企业证书举报电话：010-63071251\\xa0\\xa0\\xa0\\xa0举报邮箱：ckxx@xinhua.org- 参考消息报社 版权所有 - 站长统计',\n",
              " '参考消息第一关注中国国际观点锐参考体育健康科技应用文化旅游参考漫谈参考智库军事民族品牌参考人物产经地方译名文娱我的位置：首页>观点>新闻详情美媒：以色列正“不受制约”加紧重塑中东格局2025-06-18 11:09:2632253分享到：用微信扫描二维码参考消息网6月18日报道 美国《华尔街日报》网站6月15日刊登题为《以色列加紧重塑中东格局，且几乎不受制约》的文章，作者是安德鲁·道尔。文章编译如下：就在一年前，以色列还处境艰难——深陷加沙战争的泥潭、被伊朗支持的敌人包围，并面临华盛顿要求其停止战争的压力。如今，随着以色列领导人加大对伊朗的打击力度，它正在按照自己的方式重塑中东格局，并迫使特朗普政府跟进。这些行动可能会颠覆全球市场，重塑地缘政治格局，甚至有可能将美国拖入一场地区冲突。通过展开一系列大胆的情报行动和猛烈的军事进攻，以色列已经削弱了伊朗盟友哈马斯和真主党，同时也促使叙利亚的巴沙尔政权倒台。如今，它的战斗目标直指德黑兰。以色列利用美国的外交努力作为掩护，对伊朗发动了突袭。其目标远不止打击伊朗的核计划，还旨在重创该国政权。这场战斗也使美国的政策偏离了特朗普今年早些时候制定的路线。在长期推动通过和平外交解决方案来阻止伊朗发展核武器之后，特朗普称赞了以色列的空袭，并在社交媒体上警告说：“伊朗必须达成协议，否则将失去一切。”特朗普此前曾承诺让美国从中东冲突中脱身。但如今，他下令让美国军舰和战斗机保卫以色列，以应对来自伊朗的反击。伊朗任何针对美国军事设施或阻断波斯湾石油运输的举动都可能使美国进一步卷入这场冲突。以色列在该地区的大胆行动正值美国日益关注国内问题和来自其他地方的地缘政治威胁之际。特朗普在制定自己对中东的外交政策方针时，摒弃了美国数十年来奉行的外交惯例和优先事项。他在中东地区的尝试虽然雄心勃勃，但却反复无常。在其上任前，特朗普感兴趣的是加沙停火协议。但随着乌克兰和伊朗的局势升温，他对加沙冲突的关注度有所减弱。特朗普曾想要重新开发加沙地带，使之成为一个旅游目的地，并消灭也门的胡塞武装分子，但后来却对这些计划失去了兴趣。这为内塔尼亚胡这样目标明确的领导人强力推进其愿景打开了大门。美国前情报官员、现任职于大西洋理事会的乔纳森·帕尼科夫表示：“以色列和伊朗的未来都取决于伊朗在这场冲突结束时是否还拥有核计划。如果伊朗仍拥有核计划，其重建和在该地区投射影响力的能力将基本完好无损。反之，这将开启一个我们二十多年来从未见过的新时代。”（编译/杨雪蕾）更多新闻澳前外长：“澳大利亚应期待澳英美联盟崩溃”2025-06-18 16:23美媒：以色列正“不受制约”加紧重塑中东格局2025-06-18 11:09美媒：政治暴力频发凸显美国两极分化2025-06-17 17:05港大教授：教育有助消弭中美之间的误解2025-06-17 15:39美专家：“美国优先”成日本最大担忧2025-06-17 15:38美媒：G7凸显美国与盟友裂痕日益加深2025-06-17 11:24日媒：美一家独大导致G7难以发挥作用2025-06-17 11:23西媒：“文化战”加剧美国社会分裂2025-06-16 19:26美专家：为什么美国制造业回流并不容易2025-06-16 18:42美国外交关系协会名誉会长理查德·哈斯：中东冲突进入新阶段2025-06-16 18:42点击加载更多专题 2025企业品牌国际传播论坛 二战回眸与反思 第十五届北京国际电影节 2025世界看两会 聚焦俄乌冲突三周年排行榜外媒分析：以色列为何此时对伊朗“动手”美国：两场“大戏”即将同时上场！外媒：伊朗和以色列军队孰强孰弱？突发！以色列对伊朗发动打击美媒：美政府取消四国数十万移民合法身份中国成功斡旋，“化解宿敌”美媒：印航波音客机坠毁原因何在？伊朗：“友好”国家已就以色列动武向德黑兰发出警报针对以色列袭击伊朗，国际社会纷纷发声西媒：论中国的长远思维与五年规划关于参考消息|版权声明|广告服务|联系我们国新网备2012001\\xa0\\xa0 互联网新闻信息服务许可证\\xa0\\xa0京ICP备11013708\\xa0\\xa0京公网安备11040202440054\\xa0\\xa0互联网药品信息服务资格证书(京)-非经营性-2019-0121\\xa0\\xa0广播电视节目制作经营许可证\\xa0\\xa0中华人民共和国增值电信业务经营许可证\\xa0\\xa0高新技术企业证书举报电话：010-63071251\\xa0\\xa0\\xa0\\xa0举报邮箱：ckxx@xinhua.org- 参考消息报社 版权所有 - 站长统计',\n",
              " '参考消息第一关注中国国际观点锐参考体育健康科技应用文化旅游参考漫谈参考智库军事民族品牌参考人物产经地方译名文娱我的位置：首页>观点>新闻详情美媒：政治暴力频发凸显美国两极分化2025-06-17 17:05:0924197分享到：用微信扫描二维码参考消息网6月17日报道 美国《纽约时报》网站6月14日刊登题为《就像校园枪击案一样，政治暴力几乎成为家常便饭》的文章，作者为莉萨·莱雷尔。内容编译如下：14日，明尼苏达州一位议员及其丈夫遇刺身亡，另一位议员及其妻子遭未遂谋杀，令人毛骨悚然。震惊和哀悼的声明接踵而至。仅在过去三个月里，一名男子在宾夕法尼亚州州长官邸纵火，当时夏皮罗和他的家人正在屋内睡觉；另一名男子在华盛顿的一次活动外枪杀了两名以色列大使馆工作人员；在科罗拉多州博尔德，呼吁释放以色列人质的抗议者遭纵火袭击受伤；新墨西哥州共和党总部和阿尔伯克基附近的一家特斯拉经销店遭到燃烧弹袭击。政治暴力正缓慢而坚定地从边缘走向不可避免的现实。暴力威胁，甚至暗杀，无论未遂还是得逞，都已成为政治局面的一部分，也成为美国生活中一股持续不断的暗流。这凸显出当今美国政治暴力的双重性。就像校园枪击案一样，它既令人作呕，又几乎成了家常便饭，这也是生活在一个焦虑且危险地两极分化的国家的另一个现实。总统本人在去年竞选期间遭遇了两次暗杀，一次是在宾夕法尼亚州巴特勒的一个演讲中，一颗子弹擦过他的耳朵；另一次是在佛罗里达州，一名男子在特朗普的高尔夫球场外持半自动步枪跟踪他。去年，针对立法者的暴力威胁连续第二年创下历史新高。州和地方选举官员以及联邦法官、检察官和其他法院官员，都成为暴力威胁和骚扰的目标。根据美国普林斯顿大学弥合分歧计划收集的数据，截至4月，今年近40个州已发生170多起针对地方官员的威胁和骚扰事件。即使在实际暴力事件发生的间隙，空气中也弥漫着暴力和威胁性的政治言论。在过去的五天里，一位参议员因在新闻发布会上试图向一位内阁部长提问而被按倒在地并戴上手铐，一位州长受到总统的逮捕威胁，众议院议长还威胁要将他“涂满焦油并裹上羽毛”。当坦克准备驶过华盛顿的宪法大道，进行政治火力展示时，总统警告说，任何在那里抗议的人都将遭到“强力镇压”。对14日明尼苏达州枪击事件的回应如同往常一样。两党领导人发表声明，谴责最新发生的事件，并为受害者祈祷。随后，人们呼吁加强安保。参议院少数党领袖、民主党人查克·舒默警告称，不要止步于仅仅谴责枪击事件。他说：“谴责暴力，但忽视助长暴力的因素是不够的。我们必须采取更多措施，保护彼此，保护我们的民主，保护将我们凝聚为美国人的价值观。”自建国以来，政治暴力一直是美国历史的一部分，往往在重大变革时期爆发。有四位总统在任内遇害，还有一位总统中枪并身受重伤。几个世纪以来，国会议员卷入了数十起斗殴、决斗和其他暴力事件。特朗普在这方面也扮演了一定的角色。自首次参选总统以来，他至少暗示过默许针对政治对手的暴力行为。他鼓励集会参加者“狠狠地击倒”抗议者，赞扬一位用身体猛烈撞击记者的议员，并在2021年1月6日为那些高喊“绞死迈克·彭斯”的骚乱者辩护。他第二任期的首批举措之一就是赦免这些骚乱者。在“不要国王”抗议活动在美国各地爆发的时候，枪击事件的影响已经以实际方式延伸到政治领域。在明尼苏达州，对枪手的追捕正在进行中，执法官员敦促人们“为谨慎起见”不要参加抗议活动。在得克萨斯州奥斯汀，州警察在收到针对计划参加14日晚抗议活动的议员的可信威胁后，关闭了州议会大厦及其周边区域。约翰斯·霍普金斯大学研究暴力和政治党争的政治学家利利亚娜·梅森说：“政治暴力的目的之一是压制反对派。这不仅仅是针对少数人或受害者的行为。相比较身体伤害，其目的在于让更多的人噤声。”（编译/张琳）更多新闻澳前外长：“澳大利亚应期待澳英美联盟崩溃”2025-06-18 16:23美媒：以色列正“不受制约”加紧重塑中东格局2025-06-18 11:09美媒：政治暴力频发凸显美国两极分化2025-06-17 17:05港大教授：教育有助消弭中美之间的误解2025-06-17 15:39美专家：“美国优先”成日本最大担忧2025-06-17 15:38美媒：G7凸显美国与盟友裂痕日益加深2025-06-17 11:24日媒：美一家独大导致G7难以发挥作用2025-06-17 11:23西媒：“文化战”加剧美国社会分裂2025-06-16 19:26美专家：为什么美国制造业回流并不容易2025-06-16 18:42美国外交关系协会名誉会长理查德·哈斯：中东冲突进入新阶段2025-06-16 18:42点击加载更多专题 2025企业品牌国际传播论坛 二战回眸与反思 第十五届北京国际电影节 2025世界看两会 聚焦俄乌冲突三周年排行榜外媒分析：以色列为何此时对伊朗“动手”美国：两场“大戏”即将同时上场！外媒：伊朗和以色列军队孰强孰弱？突发！以色列对伊朗发动打击美媒：美政府取消四国数十万移民合法身份中国成功斡旋，“化解宿敌”美媒：印航波音客机坠毁原因何在？伊朗：“友好”国家已就以色列动武向德黑兰发出警报针对以色列袭击伊朗，国际社会纷纷发声西媒：论中国的长远思维与五年规划关于参考消息|版权声明|广告服务|联系我们国新网备2012001\\xa0\\xa0 互联网新闻信息服务许可证\\xa0\\xa0京ICP备11013708\\xa0\\xa0京公网安备11040202440054\\xa0\\xa0互联网药品信息服务资格证书(京)-非经营性-2019-0121\\xa0\\xa0广播电视节目制作经营许可证\\xa0\\xa0中华人民共和国增值电信业务经营许可证\\xa0\\xa0高新技术企业证书举报电话：010-63071251\\xa0\\xa0\\xa0\\xa0举报邮箱：ckxx@xinhua.org- 参考消息报社 版权所有 - 站长统计']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zwga_M1arP6"
      },
      "outputs": [],
      "source": [
        "save_to_csv(sublinks, contents, \"generalColumns_sublink_contents.csv\", tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsx_mZZ5iYSR"
      },
      "source": [
        "#### simulate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDV60qU8XthM"
      },
      "source": [
        "##### index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3uoEPW2k6Hj"
      },
      "outputs": [],
      "source": [
        "def cankaoxiaoxi_index_visiting(sublinks, contents, url):\n",
        "    # driver = get_fresh_driver()\n",
        "    # safe_get(driver, url)\n",
        "    driver = restart_driver_and_get(url)\n",
        "    wait_for_stable_page(driver)\n",
        "\n",
        "\n",
        "    num_articles = len(driver.find_elements(By.CLASS_NAME, \"clickPart\"))\n",
        "    original_url = driver.current_url\n",
        "    cnt = 0\n",
        "    while cnt < num_articles:\n",
        "        try:\n",
        "            # print(\"Enter Index Page\")\n",
        "            original_window = driver.current_window_handle\n",
        "            cards = driver.find_elements(By.CLASS_NAME, \"clickPart\")\n",
        "            # print(driver.current_url)\n",
        "            cards[cnt].click()\n",
        "\n",
        "            for handle in driver.window_handles:\n",
        "                if handle != original_window:\n",
        "                    # print(\"Switching!!\")\n",
        "                    driver.switch_to.window(handle)\n",
        "                    break\n",
        "            wait_for_stable_page(driver)\n",
        "            article_url = driver.current_url\n",
        "            sublinks.append(article_url)\n",
        "            # print(article_url)\n",
        "            # Scrape the article\n",
        "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "            # article = soup.find(\"div\", class_=\"detailsPage\")\n",
        "            # print(article.get_text(strip=True) if article else \"No article found\")\n",
        "            article = soup.get_text()\n",
        "            print(driver.title)\n",
        "            print(article)\n",
        "            contents.append(article)\n",
        "\n",
        "            # Go back to the list page\n",
        "            driver.close()\n",
        "            driver.switch_to.window(original_window)\n",
        "            # print(driver.current_url)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            try:\n",
        "                # safe_get(driver, url)\n",
        "                driver = restart_driver_and_get(url)\n",
        "                wait_for_stable_page(driver)\n",
        "            except:\n",
        "                print(f\"[{cnt}] Retry failed. Aborting.\")\n",
        "                break\n",
        "        cnt += 1\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLtQQNPANQdG"
      },
      "source": [
        "用更多新闻来截断\n",
        "From 首页>新闻详情 to 更多新闻"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efokmttWXvuH"
      },
      "source": [
        "##### generalColumns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkct6GTeSx4N"
      },
      "outputs": [],
      "source": [
        "def cankaoxiaoxi_generalColumns_visiting(sublinks, contents, tags, tag, col_url, target_class, load_button_clicks=3):\n",
        "    # driver = get_fresh_driver()\n",
        "    # safe_get(driver, url)\n",
        "    driver = restart_driver_and_get(col_url)\n",
        "    wait_for_stable_page(driver)\n",
        "\n",
        "\n",
        "    load_button = driver.find_elements(By.CLASS_NAME, \"generalColumns-loadMore\")\n",
        "    if load_button:\n",
        "      load_button = load_button[0]\n",
        "      for _ in range(load_button_clicks):\n",
        "        load_button.click()\n",
        "        wait_for_stable_page(driver)\n",
        "        load_button = driver.find_elements(By.CLASS_NAME, \"generalColumns-loadMore\")[0]\n",
        "        # print(len(driver.find_elements(By.CLASS_NAME, target_class)))\n",
        "\n",
        "    num_articles = len(driver.find_elements(By.CLASS_NAME, target_class))\n",
        "    cnt = 0\n",
        "    while cnt < num_articles:\n",
        "        try:\n",
        "            print(\"Enter Page\")\n",
        "            original_window = driver.current_window_handle\n",
        "            cards = driver.find_elements(By.CLASS_NAME, target_class)\n",
        "            print(driver.current_url)\n",
        "            cards[cnt].click()\n",
        "\n",
        "            for handle in driver.window_handles:\n",
        "                if handle != original_window:\n",
        "                    print(\"Switching!!\")\n",
        "                    driver.switch_to.window(handle)\n",
        "                    break\n",
        "            wait_for_stable_page(driver)\n",
        "            article_url = driver.current_url\n",
        "            sublinks.append(article_url)\n",
        "            tags.append(tag)\n",
        "            # Scrape the article\n",
        "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "            # article = soup.find(\"div\", class_=\"detailsPage\")\n",
        "            # print(article.get_text(strip=True) if article else \"No article found\")\n",
        "            article = soup.get_text()\n",
        "            print(driver.title)\n",
        "            print(article)\n",
        "            contents.append(article)\n",
        "\n",
        "\n",
        "            # Go back to the list page\n",
        "            driver.close()\n",
        "            driver.switch_to.window(original_window)\n",
        "            # print(driver.current_url)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            try:\n",
        "                # safe_get(driver, url)\n",
        "                driver = restart_driver_and_get(url)\n",
        "                wait_for_stable_page(driver)\n",
        "            except:\n",
        "                print(f\"[{cnt}] Retry failed. Aborting.\")\n",
        "                break\n",
        "        cnt += 1\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Frhb9oOMT0"
      },
      "source": [
        "### post process of contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vHSzarAOOM8"
      },
      "outputs": [],
      "source": [
        "load_csv(\"generalColumns_sublink_contents.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCmJvTMiRXx"
      },
      "source": [
        "#### clickpart 不行，他是用event listener处理的 需要simulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ITXu8tyohypP",
        "outputId": "f484fac1-54a0-4212-cf2d-2d7da7bd1524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.16\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.17\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.18\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.19\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.20\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.21\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.22\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.23\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.24\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.25\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.26\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.27\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.28\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.29\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.30\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.31\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.32\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.33\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.34\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.35\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.36\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.37\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.38\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.39\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.40\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.41\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.42\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.43\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.44\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.45\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.46\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.47\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.48\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.49\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.50\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.51\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.52\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.53\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.54\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.55\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.56\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.57\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.58\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.59\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.60\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.61\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.62\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.63\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.64\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.65\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.66\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.67\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.68\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.69\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.70\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.71\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.72\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.73\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.74\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.75\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.76\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.77\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.78\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.79\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.80\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.81\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.82\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.83\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.84\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.85\")>\n",
            "<selenium.webdriver.remote.webelement.WebElement (session=\"dc738d92e277d753d945b6fb1ba3a802\", element=\"f.C8121B418917C39CE0D66451E3F77314.d.70661F740BDBB11AD002FC9110D536F9.e.86\")>\n"
          ]
        }
      ],
      "source": [
        "cards = driver.find_elements(By.CLASS_NAME, \"clickPart\")\n",
        "for card in cards:\n",
        "    print(card)\n",
        "    onclick = card.get_attribute(\"onclick\")\n",
        "    if onclick:\n",
        "        print(\"onclick JS:\", onclick)\n",
        "for card in soup.select(\".clickPart\"):\n",
        "    print(1)\n",
        "    a = card.find(\"a\")\n",
        "    if a and a.get(\"href\"):\n",
        "        print(\"Title:\", card.select_one(\".content_title\").text.strip())\n",
        "        print(\"URL:\", a[\"href\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1JJI1zLNWuA"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPXmIR_WJZ9h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QW1XcHw1yIrm",
        "cW-t_orAfEos",
        "XDV60qU8XthM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}